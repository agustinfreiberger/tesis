{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5izLE2ZyfOTG"
   },
   "source": [
    "# Transfer Learning From Pre-Trained Model (VGG16)  for Face Recognition\n",
    "\n",
    "### Loading the VGG16 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "CHdrEriUfOTI",
    "outputId": "18dc7aeb-8f6c-42c6-d41b-6356b7106973"
   },
   "outputs": [],
   "source": [
    "from keras.applications import VGG16\n",
    "\n",
    "# VGG16 was designed to work on 224 x 224 pixel input images sizes but we are setting image size 100x100 to reduce computing power\n",
    "img_rows = 224\n",
    "img_cols = 224\n",
    "\n",
    "#Loads the VGG16 model \n",
    "model = VGG16(weights = 'imagenet', \n",
    "                 include_top = False, \n",
    "                 input_shape = (img_rows, img_cols, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PzoPPxJZfOTQ"
   },
   "source": [
    "### Inpsecting each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "id": "eQQCllA6fOTT",
    "outputId": "4c3412fe-3170-45a0-fc6e-ccb86b52aa3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 InputLayer True\n",
      "1 Conv2D True\n",
      "2 Conv2D True\n",
      "3 MaxPooling2D True\n",
      "4 Conv2D True\n",
      "5 Conv2D True\n",
      "6 MaxPooling2D True\n",
      "7 Conv2D True\n",
      "8 Conv2D True\n",
      "9 Conv2D True\n",
      "10 MaxPooling2D True\n",
      "11 Conv2D True\n",
      "12 Conv2D True\n",
      "13 Conv2D True\n",
      "14 MaxPooling2D True\n",
      "15 Conv2D True\n",
      "16 Conv2D True\n",
      "17 Conv2D True\n",
      "18 MaxPooling2D True\n"
     ]
    }
   ],
   "source": [
    "# Let's print our layers \n",
    "for (i,layer) in enumerate(model.layers):\n",
    "    print(str(i) + \" \"+ layer.__class__.__name__, layer.trainable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qObr05JsfOTZ"
   },
   "source": [
    "### Let's freeze all layers except the top 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "id": "mfb5fohlfOTb",
    "outputId": "52b30070-87e6-4006-d191-850499f71fa6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 InputLayer False\n",
      "1 Conv2D False\n",
      "2 Conv2D False\n",
      "3 MaxPooling2D False\n",
      "4 Conv2D False\n",
      "5 Conv2D False\n",
      "6 MaxPooling2D False\n",
      "7 Conv2D False\n",
      "8 Conv2D False\n",
      "9 Conv2D False\n",
      "10 MaxPooling2D False\n",
      "11 Conv2D False\n",
      "12 Conv2D False\n",
      "13 Conv2D False\n",
      "14 MaxPooling2D False\n",
      "15 Conv2D False\n",
      "16 Conv2D False\n",
      "17 Conv2D False\n",
      "18 MaxPooling2D False\n"
     ]
    }
   ],
   "source": [
    "# Layers are set to trainable as True by default\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "# Let's print our layers \n",
    "for (i,layer) in enumerate(model.layers):\n",
    "    print(str(i) + \" \"+ layer.__class__.__name__, layer.trainable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "POP6pYEMfOTg"
   },
   "source": [
    "### Let's make a function that returns our FC Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6_FuqNsBfOTg"
   },
   "outputs": [],
   "source": [
    "def addTopModel(bottom_model, num_classes, D=256):\n",
    "    \"\"\"creates the top or head of the model that will be \n",
    "    placed ontop of the bottom layers\"\"\"\n",
    "    top_model = bottom_model.output\n",
    "    top_model = Flatten(name = \"flatten\")(top_model)\n",
    "    top_model = Dense(512,activation='relu')(top_model)\n",
    "    top_model = Dense(256,activation='relu')(top_model)\n",
    "    top_model = Dropout(0.3)(top_model)\n",
    "    top_model = Dense(num_classes, activation = \"softmax\")(top_model)\n",
    "    return top_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jVjp2ONKfOTl",
    "outputId": "f96a15c6-1120-480d-87a3-e17de5af9ea1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input_1:0' shape=(None, 224, 224, 3) dtype=float32>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "id": "qF7jczUOfOTr",
    "outputId": "ad979357-3db3-4519-97df-3ff73819af56"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.engine.input_layer.InputLayer at 0x27ac7326648>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x27ac73f4288>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x27ac73fe048>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x27ac7541b88>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x27ac7549188>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x27acd7d6a48>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x27ac755cc88>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x27acd7dc348>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x27acd7e8b08>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x27acd7ee8c8>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x27ac754b888>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x27acd7f9d88>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x27acd809bc8>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x27acd930548>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x27acd93c708>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x27acd934688>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x27acd944cc8>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x27acd94d288>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x27acd955588>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pE9xt5XhfOTw"
   },
   "source": [
    "### Let's add our FC Head back onto VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 986
    },
    "colab_type": "code",
    "id": "wQkLRcPFfOTw",
    "outputId": "2cbae00d-b415-40ff-9047-8568202c1eea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               12845568  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 27,692,869\n",
      "Trainable params: 12,978,181\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "\n",
    "num_classes = 5\n",
    "\n",
    "FC_Head = addTopModel(model, num_classes)\n",
    "\n",
    "modelnew = Model(inputs=model.input, outputs=FC_Head)\n",
    "\n",
    "print(modelnew.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3cQslr5ifOT3"
   },
   "source": [
    "### Loading our crap Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bjmm0zZpCW_E"
   },
   "source": [
    "\n",
    "!cp -r '/content/drive/My Drive/Tesis/Dataset-resized-224.rar (Unzipped Files)/trainingSet' '/content/trainingSet'\n",
    "!cp -r '/content/drive/My Drive/Tesis/Dataset-resized-224.rar (Unzipped Files)/validationSet' '/content/validationSet'\n",
    "!cp -r '/content/drive/My Drive/Tesis/Dataset-resized-224.rar (Unzipped Files)/testSet' '/content/testSet'\n",
    "\n",
    "train_data_dir = '/content/trainingSet'\n",
    "validation_data_dir = '/content/validationSet'\n",
    "test_data_dir = '/content/testSet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z8K_HC7s8AOZ"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_data_dir = 'C:/Users/Agustin Pitaro/Documents/Tesis/Proyecto Final/Dataset/train'\n",
    "validation_data_dir = 'C:/Users/Agustin Pitaro/Documents/Tesis/Proyecto Final/Dataset/validation'\n",
    "test_data_dir = 'C:/Users/Agustin Pitaro/Documents/Tesis/Proyecto Final/Dataset/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "jsltXvJjfOT4",
    "outputId": "a0b01365-4a70-4db4-c1e4-fa51eac33b4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2800 images belonging to 5 classes.\n",
      "Found 600 images belonging to 5 classes.\n",
      "Found 600 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=20,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    " \n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    " \n",
    "# Change the batchsize according to your system RAM\n",
    "train_batchsize = 64\n",
    "val_batchsize = 32\n",
    " \n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=train_batchsize,\n",
    "        class_mode='categorical')\n",
    " \n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=val_batchsize,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_data_dir,\n",
    "    target_size=(img_rows, img_cols),\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=1,\n",
    "    class_mode=None,\n",
    "    shuffle=False,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kmHSbSAKfOT9"
   },
   "source": [
    "### Training our top layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SqdxGb4t8PoS"
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "                   \n",
    "checkpoint = ModelCheckpoint(\"vgg16-transfer-learning-1.h5\",\n",
    "                             monitor=\"val_loss\",\n",
    "                             mode=\"min\",\n",
    "                             save_best_only = True,\n",
    "                             verbose=1)\n",
    "\n",
    "earlystop = EarlyStopping(monitor = 'val_loss', \n",
    "                          min_delta = 0, \n",
    "                          patience = 3,\n",
    "                          verbose = 1,\n",
    "                          restore_best_weights = True)\n",
    "\n",
    "# we put our call backs into a callback list\n",
    "callbacks = [earlystop, checkpoint]\n",
    "\n",
    "# Note we use a very small learning rate \n",
    "#modelnew.compile(loss = 'categorical_crossentropy',\n",
    "#              optimizer = RMSprop(lr = 0.001),\n",
    "#              metrics = ['accuracy'])\n",
    "\n",
    "nb_train_samples=train_generator.n//train_generator.batch_size\n",
    "nb_validation_samples=validation_generator.n//validation_generator.batch_size\n",
    "epochs = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam\n",
      "WARNING:tensorflow:From <ipython-input-12-10bacef9ca33>:16: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/100\n",
      "43/43 [==============================] - ETA: 0s - loss: 2.2265 - accuracy: 0.3637\n",
      "Epoch 00001: val_loss improved from inf to 1.01845, saving model to vgg16-transfer-learning-1.h5\n",
      "43/43 [==============================] - 31s 712ms/step - loss: 2.2265 - accuracy: 0.3637 - val_loss: 1.0185 - val_accuracy: 0.5990\n",
      "Epoch 2/100\n",
      "43/43 [==============================] - ETA: 0s - loss: 1.0738 - accuracy: 0.5636\n",
      "Epoch 00002: val_loss improved from 1.01845 to 0.87903, saving model to vgg16-transfer-learning-1.h5\n",
      "43/43 [==============================] - 28s 658ms/step - loss: 1.0738 - accuracy: 0.5636 - val_loss: 0.8790 - val_accuracy: 0.6510\n",
      "Epoch 3/100\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.9631 - accuracy: 0.6082\n",
      "Epoch 00003: val_loss improved from 0.87903 to 0.81009, saving model to vgg16-transfer-learning-1.h5\n",
      "43/43 [==============================] - 29s 670ms/step - loss: 0.9631 - accuracy: 0.6082 - val_loss: 0.8101 - val_accuracy: 0.6944\n",
      "Epoch 4/100\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.8849 - accuracy: 0.6721\n",
      "Epoch 00004: val_loss improved from 0.81009 to 0.79678, saving model to vgg16-transfer-learning-1.h5\n",
      "43/43 [==============================] - 29s 667ms/step - loss: 0.8849 - accuracy: 0.6721 - val_loss: 0.7968 - val_accuracy: 0.7101\n",
      "Epoch 5/100\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.7778 - accuracy: 0.7014\n",
      "Epoch 00005: val_loss did not improve from 0.79678\n",
      "43/43 [==============================] - 28s 650ms/step - loss: 0.7778 - accuracy: 0.7014 - val_loss: 0.8073 - val_accuracy: 0.6875\n",
      "Epoch 6/100\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.8240 - accuracy: 0.6838\n",
      "Epoch 00006: val_loss did not improve from 0.79678\n",
      "43/43 [==============================] - 28s 655ms/step - loss: 0.8240 - accuracy: 0.6838 - val_loss: 0.8405 - val_accuracy: 0.6719\n",
      "Epoch 7/100\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.7887 - accuracy: 0.6933\n",
      "Epoch 00007: val_loss improved from 0.79678 to 0.75502, saving model to vgg16-transfer-learning-1.h5\n",
      "43/43 [==============================] - 29s 668ms/step - loss: 0.7887 - accuracy: 0.6933 - val_loss: 0.7550 - val_accuracy: 0.7188\n",
      "Epoch 8/100\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.7113 - accuracy: 0.7281\n",
      "Epoch 00008: val_loss improved from 0.75502 to 0.72177, saving model to vgg16-transfer-learning-1.h5\n",
      "43/43 [==============================] - 29s 668ms/step - loss: 0.7113 - accuracy: 0.7281 - val_loss: 0.7218 - val_accuracy: 0.7188\n",
      "Epoch 9/100\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.6961 - accuracy: 0.7259\n",
      "Epoch 00009: val_loss improved from 0.72177 to 0.67710, saving model to vgg16-transfer-learning-1.h5\n",
      "43/43 [==============================] - 29s 665ms/step - loss: 0.6961 - accuracy: 0.7259 - val_loss: 0.6771 - val_accuracy: 0.7396\n",
      "Epoch 10/100\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.5977 - accuracy: 0.7716\n",
      "Epoch 00010: val_loss did not improve from 0.67710\n",
      "43/43 [==============================] - 28s 654ms/step - loss: 0.5977 - accuracy: 0.7716 - val_loss: 0.7008 - val_accuracy: 0.7413\n",
      "Epoch 11/100\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.6204 - accuracy: 0.7701\n",
      "Epoch 00011: val_loss did not improve from 0.67710\n",
      "43/43 [==============================] - 28s 647ms/step - loss: 0.6204 - accuracy: 0.7701 - val_loss: 0.7339 - val_accuracy: 0.7205\n",
      "Epoch 12/100\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.5990 - accuracy: 0.7756Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.67710\n",
      "43/43 [==============================] - 28s 652ms/step - loss: 0.5990 - accuracy: 0.7756 - val_loss: 0.7086 - val_accuracy: 0.7222\n",
      "Epoch 00012: early stopping\n"
     ]
    }
   ],
   "source": [
    "optimizers = [ 'Adam'\n",
    "        \n",
    "    \n",
    "]\n",
    "for optimizer in optimizers:\n",
    "    print(optimizer)\n",
    "    modelnew = Model(inputs=model.input, outputs=FC_Head)\n",
    "    modelnew.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "  # tensorboard = TensorBoard(os.path.join('log_test4',f'{optimizer}_{time.time()}'))\n",
    "    classifier = modelnew.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch = nb_train_samples,\n",
    "        epochs = epochs,\n",
    "        callbacks = callbacks,\n",
    "        validation_data = validation_generator,\n",
    "        validation_steps = nb_validation_samples)\n",
    "    modelnew.save(\"vgg16-transfer-learning-1.h5\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading saved model for prediction\n",
    "\n",
    "from keras.models import load_model\n",
    "classifier = load_model('vgg16-transfer-learning-1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "colab_type": "code",
    "id": "86YJ9BUVfOUG",
    "outputId": "6e8cfa06-c208-44c1-a973-2418ca909af0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class - metal\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "[[0.002984   0.15721013 0.7867592  0.04323382 0.00981286]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "\n",
    "from os.path import isfile, join\n",
    "\n",
    "material_tags = {\"[0]\": \"carton\", \n",
    "                      \"[1]\": \"vidrio\",\n",
    "                      \"[2]\": \"metal\",\n",
    "                      \"[3]\": \"papel\",\n",
    "                      \"[4]\": \"plastico\"\n",
    "                      }\n",
    "\n",
    "\n",
    "def getRandomImage(path):\n",
    "    \"\"\"function loads a random images from a random folder in our test path \"\"\"\n",
    "    folders = list(filter(lambda x: os.path.isdir(os.path.join(path, x)), os.listdir(path)))\n",
    "    random_directory = np.random.randint(0,len(folders))\n",
    "    path_class = folders[random_directory]\n",
    "    print(\"Class - \" + str(path_class))\n",
    "    file_path = path + \"/\" + path_class\n",
    "    file_names = [f for f in listdir(file_path) if isfile(join(file_path, f))]\n",
    "    random_file_index = np.random.randint(0,len(file_names))\n",
    "    image_name = file_names[random_file_index]\n",
    "    return cv2.imread(file_path+\"/\"+image_name)    \n",
    "\n",
    "\n",
    "input_im = getRandomImage(test_data_dir)\n",
    "input_original = input_im.copy()\n",
    "input_original = cv2.resize(input_original, None, fx=0.5, fy=0.5, interpolation = cv2.INTER_LINEAR)\n",
    "    \n",
    "input_im = cv2.resize(input_im, (224, 224), interpolation = cv2.INTER_LINEAR)\n",
    "input_im = input_im / 255.\n",
    "input_im = input_im.reshape(1,224,224,3) \n",
    "    \n",
    "    # Get Prediction\n",
    "\n",
    "res = classifier.predict(input_im, verbose = 1)\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Transfer Learning From Pre-Trained Model for Face Recognition.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "gputest",
   "language": "python",
   "name": "gputest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
